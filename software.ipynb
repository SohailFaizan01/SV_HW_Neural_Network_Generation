{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- 1. Define the BNN Model Components ---\n",
        "\n",
        "class BinaryActivation(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Binary Activation function (Sign function) with Straight-Through Estimator (STE).\n",
        "    For the forward pass, the input is binarized to -1 or 1.\n",
        "    For the backward pass, the gradient is passed through unchanged (STE).\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        # Save the input tensor for use in the backward pass.\n",
        "        ctx.save_for_backward(input)\n",
        "        # Binarize the input: if input >= 0, output is 1; otherwise, -1.\n",
        "        return input.sign()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Retrieve the input tensor saved during the forward pass.\n",
        "        input, = ctx.saved_tensors\n",
        "        # Straight-Through Estimator: The gradient from the subsequent layer\n",
        "        # is passed directly back to the previous layer, effectively treating\n",
        "        # the sign function as an identity for backpropagation.\n",
        "        grad_input = grad_output.clone()\n",
        "        return grad_input\n",
        "\n",
        "class BinarizeWeights(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Binarizes weights to -1 or 1 using Straight-Through Estimator (STE).\n",
        "    For the forward pass, weights are binarized.\n",
        "    For the backward pass, gradients are computed with respect to the full-precision weights.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        # Save the full-precision input (weights) for the backward pass.\n",
        "        ctx.save_for_backward(input)\n",
        "        # Binarize the input (weights) to -1 or 1.\n",
        "        return input.sign()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Retrieve the full-precision weights saved during the forward pass.\n",
        "        input, = ctx.saved_tensors\n",
        "        # Straight-Through Estimator: The gradient is passed directly through\n",
        "        # to the full-precision weights. This allows the optimizer to update\n",
        "        # the full-precision weights that are then binarized in the next forward pass.\n",
        "        grad_input = grad_output.clone()\n",
        "        return grad_input\n",
        "\n",
        "class BNNLinear(nn.Linear):\n",
        "    \"\"\"\n",
        "    Custom Linear layer for BNNs that binarizes its weights during the forward pass.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(BNNLinear, self).__init__(in_features, out_features, bias)\n",
        "        # Create an instance of our custom BinarizeWeights function.\n",
        "        self.binarize = BinarizeWeights.apply\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Binarize the layer's weights before performing the matrix multiplication.\n",
        "        binarized_weight = self.binarize(self.weight)\n",
        "        # Perform the standard linear operation (matrix multiplication + bias).\n",
        "        output = F.linear(input, binarized_weight, self.bias)\n",
        "        return output\n",
        "\n",
        "class FullyConnectedBNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully Connected Binary Neural Network for MNIST classification.\n",
        "    Uses custom BNNLinear layers, BatchNorm1d, and BinaryActivation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(FullyConnectedBNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Define the layers of the BNN.\n",
        "        # Each hidden layer consists of a BNNLinear, BatchNorm1d, and BinaryActivation.\n",
        "\n",
        "        # First hidden layer\n",
        "        self.fc1 = BNNLinear(input_size, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.act1 = BinaryActivation.apply # Apply sign activation after BN\n",
        "\n",
        "        # Second hidden layer\n",
        "        self.fc2 = BNNLinear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.act2 = BinaryActivation.apply # Apply sign activation after BN\n",
        "\n",
        "        # Output layer (no sign activation for the final classification output)\n",
        "        self.fc3 = BNNLinear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Flatten the input image from (batch_size, 1, 28, 28) to (batch_size, 784).\n",
        "        x = x.view(-1, self.input_size)\n",
        "\n",
        "        # Pass through the first hidden layer\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act1(x) # Apply sign activation\n",
        "\n",
        "        # Pass through the second hidden layer\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.act2(x) # Apply sign activation\n",
        "\n",
        "        # Pass through the output layer. The output of this layer will be\n",
        "        # used by the CrossEntropyLoss function, which handles softmax internally.\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# --- 2. Data Loading and Preprocessing ---\n",
        "\n",
        "# Define transformations for the MNIST dataset:\n",
        "# 1. Convert PIL Image to PyTorch Tensor.\n",
        "# 2. Normalize the tensor with mean and standard deviation specific to MNIST.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Mean and Std for MNIST dataset\n",
        "])\n",
        "\n",
        "# Load the MNIST training dataset.\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',       # Directory where data will be downloaded\n",
        "    train=True,          # Specify this is the training set\n",
        "    download=True,       # Download the dataset if not already present\n",
        "    transform=transform  # Apply the defined transformations\n",
        ")\n",
        "\n",
        "# Load the MNIST testing dataset.\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,         # Specify this is the test set\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Define batch size for data loaders.\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders for training and testing.\n",
        "# Shuffling the training data helps with generalization.\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False\n",
        ")\n",
        "\n",
        "# --- 3. Model Initialization, Loss Function, and Optimizer ---\n",
        "\n",
        "# Calculate the input size (28x28 pixels for MNIST images).\n",
        "input_size = 28 * 28\n",
        "# Number of classes for MNIST (digits 0-9).\n",
        "num_classes = 10\n",
        "\n",
        "# Initialize the Fully Connected BNN model.\n",
        "model = FullyConnectedBNN(input_size, num_classes)\n",
        "\n",
        "# Determine the device to use (GPU if available, otherwise CPU).\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # Move the model to the selected device.\n",
        "\n",
        "# Define the loss function: CrossEntropyLoss is suitable for multi-class classification.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Define the optimizer: Adam is a good choice for training BNNs due to its adaptive learning rates.\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- 4. Training and Evaluation Functions ---\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Trains the BNN model for one epoch.\n",
        "    \"\"\"\n",
        "    model.train() # Set the model to training mode.\n",
        "    running_loss = 0.0 # Initialize running loss for the epoch.\n",
        "\n",
        "    # Iterate over batches in the training data loader.\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        # Move data and targets to the specified device (CPU/GPU).\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Zero the gradients of the optimizer.\n",
        "        optimizer.zero_grad()\n",
        "        # Perform a forward pass to get model output.\n",
        "        output = model(data)\n",
        "        # Calculate the loss.\n",
        "        loss = criterion(output, target)\n",
        "        # Perform a backward pass to compute gradients.\n",
        "        loss.backward()\n",
        "        # Update model parameters using the optimizer.\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() # Accumulate the loss.\n",
        "        # Print training progress periodically.\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "    print(f\"Epoch {epoch} Training Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    \"\"\"\n",
        "    Evaluates the BNN model on the test dataset.\n",
        "    \"\"\"\n",
        "    model.eval() # Set the model to evaluation mode.\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    # Disable gradient computation during evaluation for efficiency.\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            # Sum up batch loss.\n",
        "            test_loss += criterion(output, target).item()\n",
        "            # Get the index of the predicted class with the highest probability.\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            # Count correct predictions.\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    # Calculate average test loss per sample.\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    # Print evaluation results.\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n')\n",
        "    return accuracy\n",
        "\n",
        "# --- 5. Main Training Loop ---\n",
        "\n",
        "num_epochs = 10 # Number of training epochs. This can be adjusted.\n",
        "best_accuracy = 0.0 # To keep track of the best accuracy achieved.\n",
        "\n",
        "# Run the training and testing loop for the specified number of epochs.\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch) # Train for one epoch.\n",
        "    accuracy = test(model, device, test_loader)      # Evaluate after each epoch.\n",
        "    # Check if the current accuracy is better than the best recorded accuracy.\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        # Save the model's state_dict (all learnable parameters, including weights and biases for each layer).\n",
        "        # This includes weights from fc1, fc2, fc3, and parameters from bn1, bn2.\n",
        "        # The filename includes the accuracy for easy identification.\n",
        "        torch.save(model.state_dict(), f\"best_bnn_mnist_accuracy_{best_accuracy:.2f}%.pth\")\n",
        "        print(f\"Saved new best model with accuracy: {best_accuracy:.2f}%\")\n",
        "\n",
        "print(f\"Training finished. Best Test Accuracy: {best_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcypeyWNmUyV",
        "outputId": "db5bc750-3d83-40fd-a356-532dadb1b143"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 22.589930\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 4.573322\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 4.832949\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 3.339968\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.768884\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.834566\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 1.774077\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 1.517666\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 4.136853\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 3.152198\n",
            "Epoch 1 Training Loss: 3.3229\n",
            "\n",
            "Test set: Average loss: 0.0481, Accuracy: 8604/10000 (86%)\n",
            "\n",
            "Saved new best model with accuracy: 86.04%\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 3.651998\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 2.870241\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 5.000645\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.791273\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 4.600000\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.189727\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 5.619601\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 2.073589\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.621470\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 4.008524\n",
            "Epoch 2 Training Loss: 2.8407\n",
            "\n",
            "Test set: Average loss: 0.0397, Accuracy: 8688/10000 (87%)\n",
            "\n",
            "Saved new best model with accuracy: 86.88%\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.351883\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 1.220920\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 4.422818\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 4.489480\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 3.092269\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 5.975409\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 5.338584\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 4.337505\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 1.066774\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 4.346408\n",
            "Epoch 3 Training Loss: 2.6802\n",
            "\n",
            "Test set: Average loss: 0.0341, Accuracy: 8865/10000 (89%)\n",
            "\n",
            "Saved new best model with accuracy: 88.65%\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.255486\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 1.069535\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 3.980828\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 3.947171\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 2.138110\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.180723\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.778213\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 2.543104\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.362429\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 3.294583\n",
            "Epoch 4 Training Loss: 2.5578\n",
            "\n",
            "Test set: Average loss: 0.0350, Accuracy: 8908/10000 (89%)\n",
            "\n",
            "Saved new best model with accuracy: 89.08%\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.702209\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 3.645835\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 1.445144\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 1.524687\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 4.171705\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.126187\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 3.431268\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 3.680408\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 3.210526\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.497260\n",
            "Epoch 5 Training Loss: 2.5386\n",
            "\n",
            "Test set: Average loss: 0.0382, Accuracy: 8641/10000 (86%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 4.860540\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.331543\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 4.563498\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 3.356939\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 2.597259\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 1.142823\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.087997\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 1.032575\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 4.416169\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 3.012578\n",
            "Epoch 6 Training Loss: 2.4336\n",
            "\n",
            "Test set: Average loss: 0.0477, Accuracy: 8580/10000 (86%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 4.595454\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 1.322601\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 2.137937\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 2.563242\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.215479\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.792935\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 2.673655\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 2.400471\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 2.496863\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 3.857611\n",
            "Epoch 7 Training Loss: 2.4345\n",
            "\n",
            "Test set: Average loss: 0.0400, Accuracy: 8588/10000 (86%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.383053\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 1.943372\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 1.462553\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 1.074961\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 2.432269\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.849153\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 3.844758\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 2.588444\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 2.199811\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.034541\n",
            "Epoch 8 Training Loss: 2.3230\n",
            "\n",
            "Test set: Average loss: 0.0313, Accuracy: 8945/10000 (89%)\n",
            "\n",
            "Saved new best model with accuracy: 89.45%\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 3.264134\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 3.650922\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 2.713446\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 1.158661\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 3.436059\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.913382\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 2.135804\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 2.544149\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 1.981573\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 2.112036\n",
            "Epoch 9 Training Loss: 2.3439\n",
            "\n",
            "Test set: Average loss: 0.0349, Accuracy: 8887/10000 (89%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 2.653540\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 3.650645\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 1.786821\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 3.552155\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 2.693695\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 2.900349\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 1.098208\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 3.527133\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 1.992591\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 1.294199\n",
            "Epoch 10 Training Loss: 2.3988\n",
            "\n",
            "Test set: Average loss: 0.0291, Accuracy: 8966/10000 (90%)\n",
            "\n",
            "Saved new best model with accuracy: 89.66%\n",
            "Training finished. Best Test Accuracy: 89.66%\n"
          ]
        }
      ]
    }
  ]
}