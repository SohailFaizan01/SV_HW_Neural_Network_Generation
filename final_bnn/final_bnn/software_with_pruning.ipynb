{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import os # Import os for directory creation\n",
        "import math # Import math for ceil for fixed point conversion\n",
        "\n",
        "# --- 1. Define the BNN Model Components ---\n",
        "\n",
        "class BinaryActivation(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Binary Activation function (Sign function) with Straight-Through Estimator (STE).\n",
        "    For the forward pass, the input is binarized to -1 or 1.\n",
        "    For the backward pass, the gradient is passed through unchanged (STE).\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.sign()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        grad_input = grad_output.clone()\n",
        "        return grad_input\n",
        "\n",
        "class BinarizeWeights(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Binarizes weights to -1 or 1 using Straight-Through Estimator (STE).\n",
        "    For the forward pass, weights are binarized. If a weight is 0 due to pruning,\n",
        "    it remains 0 in the binarized output (as torch.sign(0) is 0).\n",
        "    For the backward pass, gradients are computed with respect to the full-precision weights.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        ctx.save_for_backward(input)\n",
        "        # Binarize. torch.sign(0) is 0, which correctly handles pruned weights\n",
        "        # by making them effectively 'no connection' in the forward pass.\n",
        "        return input.sign()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        input, = ctx.saved_tensors\n",
        "        # For binarized weights, the gradient is passed straight through (STE).\n",
        "        grad_input = grad_output.clone()\n",
        "        return grad_input\n",
        "\n",
        "class IntegerBiasSTE(torch.autograd.Function):\n",
        "    \"\"\"\n",
        "    Applies rounding to the bias in the forward pass for integer bias simulation,\n",
        "    and uses Straight-Through Estimator for backward pass.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, input_bias):\n",
        "        # Round the bias to the nearest integer\n",
        "        return input_bias.round()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        # Pass the gradient straight through for backpropagation to the full-precision bias\n",
        "        return grad_output.clone()\n",
        "\n",
        "class BNNLinear(nn.Linear):\n",
        "    \"\"\"\n",
        "    Custom Linear layer for BNNs that binarizes its weights during the forward pass\n",
        "    and quantizes its bias to an integer using STE.\n",
        "    It implicitly handles pruned weights (set to 0) as `torch.sign(0)` is 0.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super(BNNLinear, self).__init__(in_features, out_features, bias)\n",
        "        self.binarize = BinarizeWeights.apply\n",
        "        self.quantize_bias = IntegerBiasSTE.apply # Apply integer quantization to bias\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Binarize weights. If a weight is 0 due to pruning, its sign is 0,\n",
        "        # effectively making it a 'no connection' in the linear operation.\n",
        "        binarized_weight = self.binarize(self.weight)\n",
        "\n",
        "        # Apply integer quantization to the bias before using it in the linear operation\n",
        "        quantized_bias = self.quantize_bias(self.bias) if self.bias is not None else None\n",
        "\n",
        "        output = F.linear(input, binarized_weight, quantized_bias)\n",
        "        return output\n",
        "\n",
        "class FullyConnectedBNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Fully Connected Binary Neural Network for MNIST classification.\n",
        "    Uses custom BNNLinear layers, BatchNorm1d, and BinaryActivation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(FullyConnectedBNN, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.fc1 = BNNLinear(input_size, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.act1 = BinaryActivation.apply\n",
        "\n",
        "        self.fc2 = BNNLinear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)\n",
        "        self.act2 = BinaryActivation.apply\n",
        "\n",
        "        self.fc3 = BNNLinear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_size)\n",
        "\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.act1(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.act2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# --- Helper Functions for Pruning and Export ---\n",
        "\n",
        "def apply_pruning_to_weights_only(model, sparsity_target):\n",
        "    \"\"\"\n",
        "    Applies magnitude pruning to the weights of BNNLinear layers (excluding the final layer).\n",
        "    Weights below the calculated threshold are set to 0.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The BNN model.\n",
        "        sparsity_target (float): The desired sparsity (fraction of weights to prune, 0.0 to 1.0).\n",
        "    \"\"\"\n",
        "    print(f\"  Applying pruning with target sparsity: {sparsity_target*100:.2f}%\")\n",
        "    with torch.no_grad(): # Ensure no gradient computation during pruning\n",
        "        for name, module in model.named_modules():\n",
        "            # Apply pruning only to weights of BNNLinear layers, and exclude the final fc3 layer\n",
        "            if isinstance(module, BNNLinear) and \"fc3\" not in name:\n",
        "                weight = module.weight.data\n",
        "                num_weights = weight.numel()\n",
        "                num_prune = int(sparsity_target * num_weights)\n",
        "\n",
        "                if num_prune <= 0 or num_prune >= num_weights: # Avoid pruning all or none if target is extreme\n",
        "                    threshold = 0.0 if num_prune <= 0 else weight.abs().max() + 1.0 # Set threshold to prune none/all\n",
        "                else:\n",
        "                    abs_weights = weight.abs().flatten()\n",
        "                    # Find the threshold: the (num_prune)-th smallest absolute weight value\n",
        "                    # torch.topk with largest=False gives smallest values. We need the last one.\n",
        "                    threshold = torch.topk(abs_weights, num_prune, largest=False).values[-1]\n",
        "\n",
        "                # Create a mask: True for weights to keep (abs value > threshold), False for weights to prune\n",
        "                pruning_mask = (weight.abs() > threshold).float()\n",
        "\n",
        "                # Apply the mask to the weight data. This sets pruned weights to 0.\n",
        "                weight.mul_(pruning_mask)\n",
        "\n",
        "                current_sparsity = (weight == 0).sum().item() / num_weights\n",
        "                print(f\"    Layer {name}: Pruned {num_prune} weights. Actual sparsity: {current_sparsity*100:.2f}%\")\n",
        "\n",
        "def export_pruned_2bit_weights(weight_tensor, layer_name, output_dir):\n",
        "    \"\"\"\n",
        "    Exports pruned and binarized weights of a single layer as a matrix of 2-bit codes.\n",
        "    Mapping: -1 -> \"00\", 1 -> \"01\", 0 (pruned) -> \"10\".\n",
        "    Each row in the file represents a row of the weight matrix, with 2-bit codes\n",
        "    separated by spaces.\n",
        "\n",
        "    Args:\n",
        "        weight_tensor (torch.Tensor): The full-precision weight tensor of the layer\n",
        "                                      which may contain 0s due to pruning.\n",
        "                                      Shape is (out_features, in_features).\n",
        "        layer_name (str): The name of the layer (e.g., 'fc1_folded_weight').\n",
        "        output_dir (str): Directory to save the .txt files.\n",
        "    \"\"\"\n",
        "    # Move to CPU for processing\n",
        "    weights_cpu = weight_tensor.cpu()\n",
        "\n",
        "    # Replace '.' in layer names with '_' for valid filenames\n",
        "    output_filename = os.path.join(output_dir, f\"{layer_name.replace('.', '_')}_2bit.txt\")\n",
        "\n",
        "    # Open the file in write mode\n",
        "    with open(output_filename, 'w') as f:\n",
        "        # Iterate over rows (output features)\n",
        "        for row in weights_cpu:\n",
        "            row_str_parts = []\n",
        "            for val in row:\n",
        "                if val == 0: # Pruned weight (full-precision value is 0)\n",
        "                    row_str_parts.append(\"10\") # Represents 'no connection'\n",
        "                elif val > 0: # Binarized to 1\n",
        "                    row_str_parts.append(\"01\")\n",
        "                else: # Binarized to -1\n",
        "                    row_str_parts.append(\"00\")\n",
        "            f.write(' '.join(row_str_parts) + '\\n')\n",
        "\n",
        "    print(f\"  Exported {layer_name} to {output_filename} (Matrix size: {weights_cpu.shape[0]}x{weights_cpu.shape[1]}, 2-bit per weight)\")\n",
        "\n",
        "def float_to_fixed_point(value, total_bits, frac_bits):\n",
        "    \"\"\"Converts a float to a fixed-point integer representation.\"\"\"\n",
        "    scaling_factor = 2**frac_bits\n",
        "    fixed_val = round(value * scaling_factor)\n",
        "\n",
        "    # Calculate min/max representable values for a signed fixed-point number\n",
        "    max_val = (1 << (total_bits - 1)) - 1\n",
        "    min_val = -(1 << (total_bits - 1))\n",
        "\n",
        "    # Clip to prevent overflow\n",
        "    clipped_val = max(min_val, min(max_val, fixed_val))\n",
        "    return int(clipped_val)\n",
        "\n",
        "def export_fixed_point_to_mem(float_tensor, param_name, output_dir, total_bits=16, frac_bits=8):\n",
        "    \"\"\"\n",
        "    Exports a float tensor (e.g., BatchNorm parameters, biases) to a .mem file\n",
        "    in fixed-point hexadecimal format.\n",
        "    \"\"\"\n",
        "    # Ensure tensor is on CPU and flatten\n",
        "    float_tensor = float_tensor.cpu().flatten()\n",
        "\n",
        "    mem_content = []\n",
        "    # Calculate the number of hexadecimal characters needed for the total_bits.\n",
        "    hex_chars = math.ceil(total_bits / 4)\n",
        "\n",
        "    for val in float_tensor:\n",
        "        # Convert the floating-point value to its fixed-point integer representation.\n",
        "        fixed_val = float_to_fixed_point(val.item(), total_bits, frac_bits)\n",
        "\n",
        "        # Convert the fixed-point integer to a hexadecimal string.\n",
        "        # For negative numbers, ensure proper two's complement representation in hex.\n",
        "        if fixed_val < 0:\n",
        "            # Mask with 2^total_bits to get two's complement representation\n",
        "            hex_string = f'{(1 << total_bits) + fixed_val:0{hex_chars}X}'\n",
        "        else:\n",
        "            hex_string = f'{fixed_val:0{hex_chars}X}'\n",
        "        mem_content.append(hex_string)\n",
        "\n",
        "    output_filename = os.path.join(output_dir, f\"{param_name.replace('.', '_')}_fixed.mem\")\n",
        "    with open(output_filename, 'w') as f:\n",
        "        for hex_val in mem_content:\n",
        "            f.write(hex_val + '\\n')\n",
        "    print(f\"  Exported {param_name} to {output_filename} ({len(mem_content)} words of {total_bits} bits fixed-point)\")\n",
        "\n",
        "\n",
        "def fold_batchnorm(linear_layer, bn_layer):\n",
        "    \"\"\"\n",
        "    Folds Batch Normalization parameters into the preceding linear layer's weights and biases\n",
        "    for inference only. The linear_layer.weight.data should already reflect any pruning.\n",
        "\n",
        "    Args:\n",
        "        linear_layer (nn.Linear): The preceding linear layer (e.g., self.fc1)\n",
        "        bn_layer (nn.BatchNorm1d): The BatchNorm layer (e.g., self.bn1)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (folded_weight, folded_bias) as PyTorch tensors.\n",
        "    \"\"\"\n",
        "    # Get parameters from PyTorch model\n",
        "    # Note: linear_layer.bias.data here will already be integer-quantized due to the BNNLinear modification\n",
        "    weight = linear_layer.weight.data # This weight will already have 0s for pruned connections\n",
        "    bias = linear_layer.bias.data if linear_layer.bias is not None else torch.zeros(weight.shape[0], device=weight.device)\n",
        "    gamma = bn_layer.weight.data\n",
        "    beta = bn_layer.bias.data\n",
        "    running_mean = bn_layer.running_mean\n",
        "    running_var = bn_layer.running_var\n",
        "    eps = bn_layer.eps\n",
        "\n",
        "    # Calculate scale factor: gamma / sqrt(variance + epsilon)\n",
        "    scale_factor = gamma / torch.sqrt(running_var + eps)\n",
        "\n",
        "    # Folded Weight: W' = W * scale_factor\n",
        "    # If W has 0s due to pruning, W' will also have 0s in those positions.\n",
        "    folded_weight = weight * scale_factor.unsqueeze(1)\n",
        "\n",
        "    # Folded Bias: B' = beta + (B - mean) * scale_factor\n",
        "    # This bias (B) is already coming from the quantized_bias in BNNLinear forward pass\n",
        "    folded_bias = beta + (bias - running_mean) * scale_factor\n",
        "\n",
        "    return folded_weight, folded_bias\n",
        "\n",
        "\n",
        "# --- 2. Data Loading and Preprocessing ---\n",
        "# Define transformations for the MNIST dataset:\n",
        "# 1. Convert PIL Image to PyTorch Tensor.\n",
        "# 2. BINARIZE the image: pixels > 0.5 (after ToTensor, pixel values are 0.0-1.0) become 1.0, else 0.0.\n",
        "#    Then map to -1.0 and 1.0 to align with BNN activations.\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Converts PIL Image to FloatTensor [0.0, 1.0]\n",
        "    # Binarize: Pixels above 0.5 become 1.0, otherwise 0.0.\n",
        "    # Then map 0.0 -> -1.0 and 1.0 -> 1.0.\n",
        "    transforms.Lambda(lambda x: (x > 0.5).float() * 2.0 - 1.0)\n",
        "])\n",
        "\n",
        "# Load the MNIST training dataset.\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',       # Directory where data will be downloaded\n",
        "    train=True,          # Specify this is the training set\n",
        "    download=True,       # Download the dataset if not already present\n",
        "    transform=transform  # Apply the defined transformations\n",
        ")\n",
        "\n",
        "# Load the MNIST testing dataset.\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=False,         # Specify this is the test set\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "# Define batch size for data loaders.\n",
        "batch_size = 64\n",
        "\n",
        "# Create data loaders for training and testing.\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --- 3. Model Initialization, Loss Function, and Optimizer ---\n",
        "\n",
        "input_size = 28 * 28\n",
        "num_classes = 10\n",
        "model = FullyConnectedBNN(input_size, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- 4. Training and Evaluation Functions ---\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch, pruning_active=False):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "\n",
        "        # Apply gradient masking for pruned weights BEFORE optimizer.step()\n",
        "        if pruning_active:\n",
        "            with torch.no_grad(): # Don't track operations for gradient masking\n",
        "                for name, param in model.named_parameters():\n",
        "                    # Only mask gradients of weights in BNNLinear layers (not biases, not BatchNorm params)\n",
        "                    # Check if the module is BNNLinear and it's a weight parameter.\n",
        "                    # We also explicitly exclude fc3 from this masking/pruning, as it's the output layer.\n",
        "                    if 'weight' in name and isinstance(model._modules.get(name.split('.')[0]), BNNLinear) and \"fc3\" not in name and param.grad is not None:\n",
        "                        # Create mask from current (potentially pruned) weights.\n",
        "                        # This mask will be 0 for weights that were set to 0 by apply_pruning_to_weights_only.\n",
        "                        pruning_mask = (param.data != 0).float()\n",
        "                        param.grad.mul_(pruning_mask) # Zero out gradients for pruned weights\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "    print(f\"Epoch {epoch} Training Loss: {running_loss / len(train_loader):.4f}\")\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n')\n",
        "    return accuracy\n",
        "\n",
        "# --- 5. Main Training Loop ---\n",
        "\n",
        "num_epochs = 10\n",
        "best_accuracy = 0.0\n",
        "best_model_path = \"\" # To store the path of the best saved .pth model\n",
        "\n",
        "# Directory for saving .mem and .txt files\n",
        "output_dir = \"bnn_pruned_weights_export\" # New directory name for this version\n",
        "os.makedirs(output_dir, exist_ok=True) # Create directory if it doesn't exist\n",
        "\n",
        "# Fixed-point parameters for exporting folded biases\n",
        "# Set FP_FRAC_BITS to 0 to ensure biases are integers\n",
        "FP_TOTAL_BITS = 16\n",
        "FP_FRAC_BITS = 0 # Forces biases to be integers during export\n",
        "\n",
        "# Pruning parameters\n",
        "pruning_sparsity = 0.1\n",
        "pruning_start_epoch = 3 # Start pruning from this epoch onwards\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    # Determine if pruning should be active for this epoch's training and weight update\n",
        "    pruning_active_this_epoch = (epoch >= pruning_start_epoch)\n",
        "    train(model, device, train_loader, optimizer, epoch, pruning_active=pruning_active_this_epoch)\n",
        "\n",
        "    if pruning_active_this_epoch:\n",
        "        # Apply pruning to the weights after each training epoch (if pruning is active)\n",
        "        apply_pruning_to_weights_only(model, pruning_sparsity)\n",
        "\n",
        "        accuracy = test(model, device, test_loader) # This accuracy now reflects integer biases and pruned weights\n",
        "    else:\n",
        "      accuracy = 0\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        # Save the best model's full state_dict\n",
        "        best_model_path = f\"best_bnn_mnist_pruned_accuracy_{best_accuracy:.2f}%.pth\"\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"Saved new best model .pth: {best_model_path}\")\n",
        "\n",
        "        print(f\"Exporting weights and biases for best model (Accuracy: {best_accuracy:.2f}%)...\")\n",
        "\n",
        "        # Ensure model is in eval mode for correct BN behavior during folding\n",
        "        model.eval()\n",
        "        # Fold batchnorm: The linear_layer.weight.data should already be pruned.\n",
        "        fc1_folded_w, fc1_folded_b = fold_batchnorm(model.fc1, model.bn1)\n",
        "        fc2_folded_w, fc2_folded_b = fold_batchnorm(model.fc2, model.bn2)\n",
        "        model.train() # Set back to train mode\n",
        "\n",
        "        # Export pruned and binarized folded weights as 2-bit plain text matrix\n",
        "        export_pruned_2bit_weights(fc1_folded_w, 'fc1_folded_weight', output_dir)\n",
        "        export_pruned_2bit_weights(fc2_folded_w, 'fc2_folded_weight', output_dir)\n",
        "\n",
        "        # Export fixed-point folded biases to .mem files (now effectively integers)\n",
        "        export_fixed_point_to_mem(fc1_folded_b, 'fc1_folded_bias', output_dir,\n",
        "                                  total_bits=FP_TOTAL_BITS, frac_bits=FP_FRAC_BITS)\n",
        "        export_fixed_point_to_mem(fc2_folded_b, 'fc2_folded_bias', output_dir,\n",
        "                                  total_bits=FP_TOTAL_BITS, frac_bits=FP_FRAC_BITS)\n",
        "\n",
        "        # For the final layer (fc3), no BN is applied, so export original weights and biases.\n",
        "        # The fc3 weights are NOT pruned by `apply_pruning_to_weights_only` by design.\n",
        "        export_pruned_2bit_weights(model.fc3.weight.data, 'fc3_weight', output_dir)\n",
        "        export_fixed_point_to_mem(model.fc3.bias.data, 'fc3_bias', output_dir,\n",
        "                                  total_bits=FP_TOTAL_BITS, frac_bits=FP_FRAC_BITS)\n",
        "\n",
        "        print(\"Finished exporting weights and biases files.\")\n",
        "\n",
        "print(f\"\\nTraining finished. Best Test Accuracy: {best_accuracy:.2f}%\")\n",
        "if best_model_path:\n",
        "    print(f\"Best model .pth saved at: {best_model_path}\")\n",
        "    print(f\"Weights and biases files saved in: {output_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcypeyWNmUyV",
        "outputId": "58d40026-8128-4805-e94d-f7ce88329a51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 19.918152\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 3.777128\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 4.072897\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 1.774574\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 4.192986\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.229078\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.546088\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.737727\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.729961\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 2.357657\n",
            "Epoch 1 Training Loss: 3.6230\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 3.519199\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 1.973618\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.879915\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 1.715782\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 4.625730\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 4.241850\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 2.870506\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 1.715907\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 4.566566\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 3.089037\n",
            "Epoch 2 Training Loss: 2.9793\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 3.213254\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 5.411490\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 2.640173\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 2.550721\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 2.234610\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 1.917457\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 3.408807\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 2.956066\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 3.513222\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 3.332006\n",
            "Epoch 3 Training Loss: 2.8803\n",
            "  Applying pruning with target sparsity: 10.00%\n",
            "    Layer fc1: Pruned 40140 weights. Actual sparsity: 10.00%\n",
            "    Layer fc2: Pruned 13107 weights. Actual sparsity: 10.00%\n",
            "\n",
            "Test set: Average loss: 0.0406, Accuracy: 8575/10000 (86%)\n",
            "\n",
            "Saved new best model .pth: best_bnn_mnist_pruned_accuracy_85.75%.pth\n",
            "Exporting weights and biases for best model (Accuracy: 85.75%)...\n",
            "  Exported fc1_folded_weight to bnn_pruned_weights_export/fc1_folded_weight_2bit.txt (Matrix size: 512x784, 2-bit per weight)\n",
            "  Exported fc2_folded_weight to bnn_pruned_weights_export/fc2_folded_weight_2bit.txt (Matrix size: 256x512, 2-bit per weight)\n",
            "  Exported fc1_folded_bias to bnn_pruned_weights_export/fc1_folded_bias_fixed.mem (512 words of 16 bits fixed-point)\n",
            "  Exported fc2_folded_bias to bnn_pruned_weights_export/fc2_folded_bias_fixed.mem (256 words of 16 bits fixed-point)\n",
            "  Exported fc3_weight to bnn_pruned_weights_export/fc3_weight_2bit.txt (Matrix size: 10x256, 2-bit per weight)\n",
            "  Exported fc3_bias to bnn_pruned_weights_export/fc3_bias_fixed.mem (10 words of 16 bits fixed-point)\n",
            "Finished exporting weights and biases files.\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.296663\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 2.326236\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 1.125079\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 1.106564\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 1.357091\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 3.495711\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 1.715876\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 3.163197\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 2.022965\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 2.634989\n",
            "Epoch 4 Training Loss: 2.7927\n",
            "  Applying pruning with target sparsity: 10.00%\n",
            "    Layer fc1: Pruned 40140 weights. Actual sparsity: 10.00%\n",
            "    Layer fc2: Pruned 13107 weights. Actual sparsity: 10.00%\n",
            "\n",
            "Test set: Average loss: 0.0381, Accuracy: 8616/10000 (86%)\n",
            "\n",
            "Saved new best model .pth: best_bnn_mnist_pruned_accuracy_86.16%.pth\n",
            "Exporting weights and biases for best model (Accuracy: 86.16%)...\n",
            "  Exported fc1_folded_weight to bnn_pruned_weights_export/fc1_folded_weight_2bit.txt (Matrix size: 512x784, 2-bit per weight)\n",
            "  Exported fc2_folded_weight to bnn_pruned_weights_export/fc2_folded_weight_2bit.txt (Matrix size: 256x512, 2-bit per weight)\n",
            "  Exported fc1_folded_bias to bnn_pruned_weights_export/fc1_folded_bias_fixed.mem (512 words of 16 bits fixed-point)\n",
            "  Exported fc2_folded_bias to bnn_pruned_weights_export/fc2_folded_bias_fixed.mem (256 words of 16 bits fixed-point)\n",
            "  Exported fc3_weight to bnn_pruned_weights_export/fc3_weight_2bit.txt (Matrix size: 10x256, 2-bit per weight)\n",
            "  Exported fc3_bias to bnn_pruned_weights_export/fc3_bias_fixed.mem (10 words of 16 bits fixed-point)\n",
            "Finished exporting weights and biases files.\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 3.564914\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 3.369546\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 2.256509\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 3.294961\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.530631\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.566805\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 3.474788\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 1.953786\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 2.377881\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 1.558915\n",
            "Epoch 5 Training Loss: 2.7212\n",
            "  Applying pruning with target sparsity: 10.00%\n",
            "    Layer fc1: Pruned 40140 weights. Actual sparsity: 10.00%\n",
            "    Layer fc2: Pruned 13107 weights. Actual sparsity: 10.00%\n",
            "\n",
            "Test set: Average loss: 0.0501, Accuracy: 8407/10000 (84%)\n",
            "\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.501196\n",
            "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 2.826303\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 3.344179\n",
            "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 1.564967\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 3.624745\n",
            "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 3.417437\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 2.205665\n",
            "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 5.681977\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 2.905028\n",
            "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 1.230159\n",
            "Epoch 6 Training Loss: 2.6192\n",
            "  Applying pruning with target sparsity: 10.00%\n",
            "    Layer fc1: Pruned 40140 weights. Actual sparsity: 10.00%\n",
            "    Layer fc2: Pruned 13107 weights. Actual sparsity: 10.00%\n",
            "\n",
            "Test set: Average loss: 0.0588, Accuracy: 7914/10000 (79%)\n",
            "\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.791573\n",
            "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 3.318423\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 4.015701\n",
            "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 3.133574\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.826549\n",
            "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 2.014842\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 3.536178\n",
            "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 3.644331\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 3.160933\n",
            "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 1.924682\n",
            "Epoch 7 Training Loss: 2.5855\n",
            "  Applying pruning with target sparsity: 10.00%\n",
            "    Layer fc1: Pruned 40140 weights. Actual sparsity: 10.00%\n",
            "    Layer fc2: Pruned 13107 weights. Actual sparsity: 10.00%\n",
            "\n",
            "Test set: Average loss: 0.0492, Accuracy: 8260/10000 (83%)\n",
            "\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.674404\n",
            "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 3.381328\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 2.473591\n",
            "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 2.859450\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 1.598055\n",
            "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.859503\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.972446\n",
            "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 3.769602\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 4.775197\n",
            "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 2.839028\n",
            "Epoch 8 Training Loss: 2.5249\n",
            "  Applying pruning with target sparsity: 10.00%\n",
            "    Layer fc1: Pruned 40140 weights. Actual sparsity: 10.00%\n",
            "    Layer fc2: Pruned 13107 weights. Actual sparsity: 10.00%\n",
            "\n",
            "Test set: Average loss: 0.0469, Accuracy: 8141/10000 (81%)\n",
            "\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.968697\n",
            "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 3.787272\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 4.516322\n",
            "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 4.548363\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.731596\n",
            "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 2.668639\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 1.970742\n",
            "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 4.953563\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 4.215727\n",
            "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 1.716192\n",
            "Epoch 9 Training Loss: 2.5242\n",
            "  Applying pruning with target sparsity: 10.00%\n",
            "    Layer fc1: Pruned 40140 weights. Actual sparsity: 10.00%\n",
            "    Layer fc2: Pruned 13107 weights. Actual sparsity: 10.00%\n",
            "\n",
            "Test set: Average loss: 0.0552, Accuracy: 8159/10000 (82%)\n",
            "\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.698920\n",
            "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 1.348412\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 2.814700\n",
            "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 2.451393\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 1.237132\n",
            "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.257859\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 3.234273\n",
            "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.971107\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 2.201494\n",
            "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 2.095892\n",
            "Epoch 10 Training Loss: 2.5256\n",
            "  Applying pruning with target sparsity: 10.00%\n",
            "    Layer fc1: Pruned 40140 weights. Actual sparsity: 10.00%\n",
            "    Layer fc2: Pruned 13107 weights. Actual sparsity: 10.00%\n",
            "\n",
            "Test set: Average loss: 0.0694, Accuracy: 7437/10000 (74%)\n",
            "\n",
            "\n",
            "Training finished. Best Test Accuracy: 86.16%\n",
            "Best model .pth saved at: best_bnn_mnist_pruned_accuracy_86.16%.pth\n",
            "Weights and biases files saved in: bnn_pruned_weights_export\n"
          ]
        }
      ]
    }
  ]
}