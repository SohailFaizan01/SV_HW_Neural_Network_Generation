# -*- coding: utf-8 -*-
"""software code_group 6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v-Okd7edjaYTBORTqTNHcNGsp2w-aldC
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
import numpy as np
import os

# ======== Binary Activation Function ========
class BinaryActivation(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.sign()

    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input.abs() > 1] = 0
        return grad_input

def binary_activation(x):
    return BinaryActivation.apply(x)

# ======== Binary Linear Layer (with BatchNorm + Binary Weight) ========
class BinaryLinear(nn.Module):
    def __init__(self, in_features, out_features):
        super(BinaryLinear, self).__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.bn = nn.BatchNorm1d(out_features)

    def forward(self, x):
        bin_weight = self.linear.weight.sign()
        x = nn.functional.linear(x, bin_weight, self.linear.bias)
        x = self.bn(x)
        return binary_activation(x)

# ======== Full Binary Neural Network (except output logits) ========
class BNN(nn.Module):
    def __init__(self):
        super(BNN, self).__init__()
        self.fc1 = BinaryLinear(28*28, 1024)
        self.fc2 = BinaryLinear(1024, 10)  # ✅ binarized weights

    def forward(self, x):
        x = x.view(-1, 28*28)
        x = self.fc1(x)
        x = self.fc2(x)
        return x  # ⚠️ Do not do sign, keep float logits

# ======== Load MNIST (Binarized Input) ========
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Lambda(lambda x: (x > 0.5).float()),  # Binarization 0/1
    transforms.Lambda(lambda x: x * 2 - 1)           # Mapped to -1/+1
])
train_loader = torch.utils.data.DataLoader(
    datasets.MNIST('./data', train=True, download=True, transform=transform),
    batch_size=128, shuffle=True
)

# ======== Training Setup ========
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = BNN().to(device)
optimizer = optim.Adam(model.parameters(), lr=0.0007)
loss_fn = nn.CrossEntropyLoss()

# ======== Training Loop ========
for epoch in range(40):
    model.train()
    total_loss = 0
    for data, target in train_loader:
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # Accuracy Evaluation
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            pred = output.argmax(dim=1)
            correct += (pred == target).sum().item()
            total += target.size(0)
    acc = 100. * correct / total
    print(f"Epoch {epoch+1}: Loss = {total_loss:.4f}, Accuracy = {acc:.2f}%")

# ======== Export Weights (binary) and Thresholds ========
os.makedirs("bnn_export", exist_ok=True)

np.savetxt("bnn_export/fc1_weights.txt", model.fc1.linear.weight.detach().sign().cpu().numpy(), fmt='%d')
np.savetxt("bnn_export/fc1_thresh.txt", model.fc1.bn.bias.detach().cpu().numpy(), fmt='%.6f')

np.savetxt("bnn_export/fc2_weights.txt", model.fc2.linear.weight.detach().sign().cpu().numpy(), fmt='%d')
np.savetxt("bnn_export/fc2_thresh.txt", model.fc2.bn.bias.detach().cpu().numpy(), fmt='%.6f')

print("✅ Export successful (fc1 + fc2): All bin weights and thresholds generated in bnn_export/ directory!")